\documentclass{article}
\usepackage[nonatbib]{project}

\usepackage[breaklinks=true,letterpaper=true,colorlinks,citecolor=black,bookmarks=false]{hyperref}

\usepackage{amsthm}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{comment}

\usepackage[sort&compress,numbers]{natbib}
\usepackage[normalem]{ulem}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig} 

\graphicspath{{../fig/}}

\usepackage{tikz}
\usepackage{tkz-tab}
\usepackage{caption} 
\usepackage{subcaption} 
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{arrow} = [very thick,->,>=stealth]

\usepackage{cleveref}
\usepackage{setspace}
\usepackage{wrapfig}
%\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage[noend,linesnumbered]{algorithm2e}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{url}
\usepackage{float}

\usepackage[disable]{todonotes}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\title{What does LSTM learn? Contextual Decomposition for Rationalizing LSTM Predictions}

\author{
	Yuqing Xie \\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{yuqing.xie@uwaterloo.ca} \\
	\And
	Peng Shi\\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{TODO@uwaterloo.ca} \\
}

\begin{document}
\maketitle
% expect the report to be less than \textbf{8 pages} (references excluded).

\begin{abstract}

Put here a brief summary of the project: what is it about, what are the related works, what is your execution plan, what do you expect to learn/contribute, and how are you going to evaluate your results. The proposal is expected to be 1 page (reference excluded), so be concise and to the point. 

\end{abstract}

\section{Introduction}

Recurrent neural networks(RNN) are now the standard method to deal with natural language. Models using long-short-term-memory(LSTM) perform extremely well on varieties of natural language tasks, ranging from machine translation, question answering to sentence classification and sentence tagging. However, unlike Convolution neural networks for vision tasks, the mechanism behind RNN is less explored and hard to visualize. Without clear understanding of the sequential models, improvements can be hard to made and further improvement will be less directed. 

Several previous work has been done to try to visualize LSTM. However, no significant findings has been made to help understand the contributions of each words. 
TODO:related works summary.
%Why can't any of the existing techniques effectively tackle this problem?

Since few theory work has been done to successfully dig deep into sequential models and apply the findings to practice, we proposed a novel method of decomposing Bi-LSTM models and visualize them for further model improvement. We decomposed trained Bi-LSTM models for sequence tagging tasks to understand the contribution of each components in the sentence. %We also improved the final model according to this finding. 
% take cross-view learning as an example in this project. We will analyze and improve cross-view learning using Bayesian ensemble.

First thought to investigate the LSTM is to watch on the contribution of each word toward the output. A clear way to do this is to apply the attention mechanism. However, what we aim to do here is not to improve the model by introducing extra weights but to just examine the existing model. We follow Jame’s to decomposition[TODO]. The main idea is to decompose the output gates in LSTM and convert them in to the contribution weights of words towards outputs. Their work focus on the sentence classification tasks -- sentiment classification. This is relative simple task and needs less understanding of the sentence structure. Contributions of each word is relatively clear according to their meaning, which can be easily represented by current word embeddings. We follow their method  made modification of their models since we focus on sequence tagging tasks and examined Bi-LSTM.

%Possible difficulties - main contribution
%How to quantify the observations and summarize it into prior knowledge.
%How to integrate the human prior knowledge in Bayesian Ensemble.

%CVT
%First let me introduce you what is cross view training. It is a semi-supervised sequence learning method. This idea first come from vision. We human can construct 3D models from slightly different views from two eyes. Even if we only look through one eye, we can still understand the world in a 3D way. That is because our brain is trained a long time to do that. Kevin Clark first brought this idea to sequence model. He took the Bi-LSTM as the primary view, which is looking through both eyes, and take each directional LSTM as partial view, which is look through only one eye. The idea of cross view training is that enabling partial views the ability to understand the whole sentence and then in tern improve the primary view, since the primary view is made up with partial views.
%Now, let’s take a closer look at cross view training. On the labeled data, Kevin followed the traditional bidirectional LSTM fashion. On the unlabeled data, Kevin followed the teacher-student fashion. We take the primary view as the teacher, and take the partial views as students. Through training, partial views will gradually learn to understand the whole sentence. However, one interesting thing here is that  in Kevin Clark’s work, they only use primary view during inference. We think the reason is that partial views are not reliable, and simply ensemble them with primary view may even add burden to the model.
%So, here comes our problem. We would like to Investigate the source of the effectiveness of cross-view training method. We will try to answer the following questions: * What does LSTM with CVT learn from the unlabeled data compared with traditional LSTM. * What is learned for different views in CVT.  And further, we wish to improve the cross-view training method based on our investigation.
%What properties did you analyze/prove about this problem or technique?

\section{Related Works}

There are mainly two lines of related work on neural network interpretability.

%TODO rewrite and summary.

First, Hendrik et al. (2016) and Karpathy et al. (2016) analyze the movement of the raw gate activations over a sequence.
Karpathy et al. (2016) is able to identify co-ordinates of $c_t$ that correspond to semantically meaningful attributes such as whether the text is in quotes and how far along the sentence a word is. However, most of the cell co-ordinates are harder to interpret, and in particular, it is often not obvious from their activations which inputs are important for specific outputs.

Another approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM’s, and show that they can give better results in that setting.

%Related work - Sequence task ensemble
%For sequential task ensemble there also exist several kinds of methods
%Rodrigues et al. (2014) proposed a CRF-based model, CRF-MA, that assumes only one annotator is correct for any given label.
%Recently, Nguyen et al. (2017) proposed an approach that outperformed CRFMA, based on hidden Markov models (HMMs),
%Both CRF-MA and HMM crowd use simpler annotator models that do not capture the effect of sequential dependencies on annotator reliability.
%Bayesian approach, which has been shown to be effective for handling uncertainty due to noise in crowdsourced data for non-sequential classification (Kim and Ghahramani, 2012; Simpson et al., 2013; Venanzi et al., 2014; Moreno et al., 2015).
%Yang et al. (2018) adapt a Bayesian neural network so that it can be trained concurrently with an annotator model.
%We will also use a Bayesian approach to aggregate primary view with partial views.

%TODO
%Summarize the range of techniques by highlighting their strengths and weaknesses  differences or limitations (if any)? 
%Suggestion: organize your summary based on desirable properties of the techniques
%What is the state of the art?
%Any open problem?

\section{Models}

To understand the mechanism behind current sequential models, we follow [TODO Ian Tenney]'s ``edge probing'' framework to construct an architectures across tasks. Unlike [Tenney] focusing on sequence classification tasks, we focus on sequence tagging problem. That is to say, given a sentence, we have to give each token a label based on the whole sentence.

\subsection{Notations}
Formally, we represent a sentence as a list of tokens $T = [t_1, t_2, ... , t_n]$, where $n$ represents the length of the sentence, and the label of each token to be $L = [l_1, l_2, ... , l_n]$. Given a sequence of tokens, we embed the tokens into a high dimension vectors. Thus, a sentence can be represented as $E = [e_1, e_2, ... , e_n], e_i \in \mathbb{R}^d, \forall 1 \leq i \leq n$, where $d$ is the embedding size. Then we pass this embedded sentence into models.

%\red{We focus on the named entity recognition task. This is to tag each word its category in terms of entity, that is does the word refer to a person, a location, an organization, etc. Usually the system needs to tag for each token their category alone with whether it is the start, inner or end part of this entity. To make future analysis easier, we simplify the task to be a binary classification task. That is to say, we classify each token to be either entity or non-entity.} 
%states twice ?


We implement several models to compare the contribution of both embedding information and contextual information to the final prediction. 


\subsection{Uni-LSTM}

We follow the traditional LSTM mechanism to construct a model for sequence tagging task. Given a sequence of word embeddings, $E = [e_1, e_2, ... , e_n], e_i \in \mathbb{R}^d$, an Uni-LSTM processes one word at a time, keeping track of cell-gates $C = [c_1, c_2, ... , c_n]$ and hidden states $H = [h_1, h_2, ... , h_n]$. After processing each token, the hidden state of current token is then passed through a fully-connected layer to form the probability distribution over classes. After a softmax activation function and argmax, the model will predict a label across classes.

In detail, each Uni-LSTM cell computes $h_t$ and $c_t$ as a function of $e_t, c_{t-1}$ using the following updates. 
\begin{align} 
f_t & = \sigma(W_f x_t + V_f h_{t - 1} + b_f) \label{eq:1f}\\
i_t & = \sigma(W_i x_t + V_i h_{t - 1} + b_i) \label{eq:1i}\\
o_t & = \sigma(W_o x_t + V_o h_{t - 1} + b_o) \label{eq:1o}\\
\tilde{c}_t & = \tanh(W_c x_t + V_c h_{t - 1} + b_c) \\
c_t & = f_t \odot c_{t - 1} + i_t \odot \tilde{c}_t \label{eq:1c}\\
h_t & = o_t \odot \tanh(c_t)\label{eq:1h}
\end{align}

As initial values, we define $c_0=h_0=0$. 
Then the probability distribution for the $t$ th token for the $c$ th class is specified by 
\begin{equation}
p_{t, c} = \text{SoftMax}(W h_t + b) = \frac{e^{W_{:c} h_t + b_{:c}}}{\sum_{j = 1} ^ C e^{W_{:j} h_t + b_{:j}}},
\end{equation}
where $W_{:j}$ is the $i$ th row of the matrix $W$ and $b_{:j}$ is the bias of the $i$ th.

This is the forward LSTM. In this case, the information is passed from the beginning of the sentence to the end. We also consider the backward LSTM for later comparison. The only difference will be the update direction and the initial value. The initial values are assigned to $c_n=h_n=0$. And the information is passed from the end of the sentence to the beginning, which means we update the $t$ th token using the $t+1$ th token instead of the $t-1$ th as in the forward version.
%\begin{align}
%f_t & = \sigma(W_f x_t + V_f h_{t + 1} + b_f) \\
%i_t & = \sigma(W_i x_t + V_i h_{t + 1} + b_i) \\
%o_t & = \sigma(W_o x_t + V_o h_{t + 1} + b_o) \\
%\tilde{c}_t & = \tanh(W_c x_t + V_c h_{t + 1} + b_c) \\
%c_t & = f_t \odot c_{t _ 1} + i_t \odot \tilde{c}_t \\
%h_t & = o_t \odot \tanh(c_t)
%\end{align}


\subsection{Bi-LSTM}

Uni-LSTM networks suffer from only seeing information from one side of t he sentence. So we also include a Bi-LSTM to examine the information learnt from both sides. The encoding process is just simply concatenate forward and backward gates together. We demote the forward gates as $f_t^{fw}, i_t^{fw}, o_t^{fw}, c_t^{fw}, h_t^{fw}$, and the backward gates as $f_t^{bw}, i_t^{bw}, o_t^{bw}, c_t^{bw}, h_t^{bw}$. We also demote the concatenated matrix of $*_t^{fw}$ and $*_t^{bw}$ as $[*_t^{fw}, *_t^{bw}]$. After concatenate the forward and backward gates together, using $[*_t^{bw}, *_t^{fw}]$ to replace $*_t$ in formulas from \ref{eq:1f} to \ref{eq:1h}, we can represent the update formulas in a same form. 

Also probability distribution for the $t$ th token for the $c$ th class is specified by 
\begin{equation}
p_{t, c} = \text{SoftMax}(W [h_t^{fw}, h_t^{bw}] + b) = \frac{e^{W_{:c} [h_t^{fw}, h_t^{bw}] + b_{:c}}}{\sum_{j = 1} ^ C e^{W_{:j} [h_t^{fw}, h_t^{bw}] + b_{:j}}}.
\end{equation}


\subsection{MLP}

To examine what information has been learnt by LSTM from the context and what information can be directly learned from the word embedding, we also implemented a m-layer MLP network to tag the tokens only according to the embeddings. Each MLP layer can be represented as follow:
The input of the $i$ th layer is represented as $x_i$, then the $i$ th layer is $f_i(x_i) = \sigma(W_i x_i + b_i)$, where $W_i$ and $b_i$ are the weights and bias of the $i$ th layer. In this work we use ReLU as our activation function $\sigma$. Then the output of the $i$ th layer $f_i(x_i)$ is then feed into the next layer as input. That is to say $x_{i+1} = f_i(x_i)$. 

We compared the four different embeddings: the original pre-trained embedding from Glove, the tuned embedding of forward and backward Uni-LSTM and the Bi-LSTM. The final probability will be $p_{t, c} = \text{SoftMax}(f_m(e_t))$.% explain more?

\subsection{Uni-LSTM Decomposition}

We now show that we can decompose the numerator of $p_{t, c}$ into a product of factors, and interpret those factors as the contribution of individual words of the predicted probability of class $c$ for Uni-LSTM. Also taking the forward direction as an example, define 
\begin{equation}\label{eq:uni-beta} 
\beta_{t, c, j} = \exp\left(W_{:c} (o_t \odot (\tanh(c_j)- \tanh(c_{j-1}))\right),
\end{equation}
so that 
\[\exp( W_{:c} h_t) = \exp\left(\sum_{j=1}^t W_{:c} (o_t \odot (\tanh(c_j) - \tanh(c_{j-1}))\right) = \prod_{j=1}^t \beta_{t, c, j}.\]

As $\tanh(c_j) - \tanh(c_{j-1})$ can be viewed as the update resulting from word $j$, so $\beta_{t, c, j}$ can be interpreted as the multiplicative contribution to $p_{t, c}$ by word $j$. 

Now, we have 
\begin{equation}
p_{t, c} \propto \exp( W_{:c} h_t + b_{:c}) = \exp(b_{:c})\prod_{j=1}^t \beta_{t, c, j}.
\end{equation}

This is a direct extension of [TODO]'s work, in which we expand their method to suit sequence tagging tasks.
The backward direction will just be the same.

\subsection{Bi-LSTM Decomposition}

We also extend the decomposition for Bi-LSTM model. For Bi-LSTM, we can simply de-concatenate the gate states and compute the contribution of each direction as in the Uni-LSTM. Then each token's probability distribution can be taken as a combination of both direction. That is to say, the probability can be interpreted as follow:
\begin{align}\label{eq:bi-beta} 
\beta_{t, c, j} = \exp(&W_{:c} [h_t^{fw}, h_t^{bw}])\\
= \exp(&W_{:c}^{fw} (o_t^{fw} \odot (\tanh(c_j^{fw})- \tanh(c_{j-1}^{fw}))) \\
+ &W_{:c}^{bw} (o_t^{bw} \odot (\tanh(c_j^{bw})- \tanh(c_{j+1}^{bw})))),
\end{align}
Let
\begin{equation}
\beta_{t, c, j}^{fw} = \exp(W_{:c}^{fw} (o_t^{fw} \odot (\tanh(c_j^{fw})- \tanh(c_{j-1}^{fw}))))
\end{equation}
and
\begin{equation}
\beta_{t, c, j}^{bw} = \exp(W_{:c}^{bw} (o_t^{bw} \odot (\tanh(c_j^{bw})- \tanh(c_{j+1}^{bw}))))
\end{equation}
so that 
\begin{equation}
\beta_{t, c, j} = \beta_{t, c, j}^{fw} \beta_{t, c, j}^{bw}
\end{equation}
Then we have:
\begin{align}
p_{t, c} \propto \exp(&W_{:c} [h_t^{fw}, h_t^{bw}] + b_{:c}) \\
= \exp(&b_{:c} + \sum_{j=1}^t W_{:c}^{fw} (o_t^{fw} \odot (\tanh(c_j^{fw}) - \tanh(c_{j-1}^{fw})))\\
	+&\sum_{j=t}^n W_{:c}^{bw} (o_t^{bw} \odot (\tanh(c_j^{bw}) - \tanh(c_{j+1}^{fw}))))\\
=  \exp(&b_{:c})\prod_{j=1}^t \beta_{t, c, j}^{fw} \prod_{j=t}^n \beta_{t, c, j}^{bw}.
\end{align}

\section{Model Comparison}

For better understanding, we break down the problem of ``What does the Bi-LSTM learn'' into three small problems:

\begin{itemize}
	\item Does Bi-LSTM perform significantly better than Uni-LSTM and local models such as MLP? 
	In what cases, Bi-LSTM, Uni-LSTM and local models work well or better than the other models?
	\item Does Bi-LSTM model learn to do prediction from contextual information? How to verify and measure the difference?
	\item Why do Uni-LSTM and local model also work in some cases? 
\end{itemize}

To answer first question, we conduct the experiments with Bi-LSTM, Uni-LSTM model. The embedding is initialized with GloVe pretraining embeddings~\cite{pennington2014glove}. To investigate what kind of information gain provided by the LSTM, we also conduct experiments with MLP. We adopt the fine-tuned embeddings in LSTM and fix them during the training of MLP model. This is known as probing technique[TODO cite]. 

To answer the second question, we use the LSTM decomposition technique to further investigate the contribution distribution for the token level prediction. The decomposition is conducted for Bi-LSTM and Uni-LSTM. 

To answer the third question, we leverage visualization techniques such as t-SNE~\cite{maaten2008visualizing}, to visualize the hidden vectors together with initial embedding vectors. T-SNE is a technique that can visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The main idea is to first using mixture models to estimate the distribution of points and then minimize the KL-divergence between point distribution before and after the mapping.

%formulate your problem precisely (mathematically), present the technical challenges (if any), discuss the tools or datasets that you will build on, state your goals, and come up with a plan for evaluation.

\section{Experimental Setup}

%We conduct extensive experiments to answer the question: What do the Bi-LSTM learn? 

\subsection{Dataset} 

Our experiments are conducted based on OntoNotes 5 English dataset~\cite{weischedel2012ontonotes}, focusing on Entity Detection task. The Entity Detection task is usually formulated as a sequence labeling task: labels are assigned to each token. Different tagging scheme can be adopted, such as \textit{BIO}, \textit{BIOES}, etc. For simplification, in this work, we use \textit{IO} scheme: if the token is a part of entity, it will be annotated as \textbf{I}; otherwise, it will be annotated as \textbf{O}. One special case that this tagging scheme can not handle is that two entity appear consecutively in one sentence, because there is not boundary label such \textbf{B}~(begin of entity). So in our experiments, we remove the sentences with consecutive entities for further experiments. The performance is evaluated by Precision, Recall and F1 scores.

\subsection{Hyper-parameters}


\section{Results}

We demonstrate our results regarding different questions we are trying to answer. Here, Uni-LSTM-forward denotes Uni-directional LSTM with forward direction; Uni-LSTM-backward has similar meanings. MLP(glove) denotes the MLP model trained with GloVe embeddings which are finetued during the learning process. MLP(bi) denotes the MLP model based on the embeddings in Bi-LSTM model. MLP(uni-forward) denotes the MLP model based on the embeddings in Uni-LSTM-forward model, and MLP(uni-backward) has similar meaning.

\subsection{Question 1}

\begin{table}[t]
	\centering
	\begin{tabular}{l@{\qquad}ccc}
		\toprule
		\textbf{Model}         & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\ \midrule
		Bi-LSTM &  84.82 & 83.60  & 84.20 \\
		Uni-LSTM-forward & 72.47 & 73.88 & 73.17 \\
		Uni-LSTM-backward & 80.31 & 80.28 & 80.29 \\ \midrule
		MLP(glove) & 60.63 & 61.82 & 61.22 \\
		MLP(bi) & 54.97 & 75.65 & 63.67 \\ 
		MLP(uni-forward) & 60.80 & 61.75 &  61.27 \\ 
		MLP(uni-backward) & 54.61 & 75.99 & 63.55 \\ 
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{Entity Detection performance on OntoNotes 5 English Dataset Test set. }

	\label{res:ner}
\end{table}



The experiment results are shown in Table~\ref{res:ner}. Compare the effectiveness of Bi-LSTM model and Uni-LSTM-forward model, we can observe that Bi-LSTM has better F1 score~(+11.03) than the Uni-LSTM model, which is a large gap in Entity Detection task. However, we can observe that the Uni-LSTM-backward perform significantly better than the Uni-LSTM-forward, with large margin(+7.12), and approaching the effectiveness of Bi-LSTM.

For the group of MLP models, we can find that they have similar performance~(around 60 F1 scores).Using the embeddings learned from the LSTM models does not provide to much gains on the effectiveness when comparing their performance with MLP(glove). Further comparing the results of Bi-LSTM and MLP(bi), we can better understand the gains brought by the Bi-LSTM contextual encoding layer. Even leveraging the embeddings learned in Bi-LSTM model, the fine-tune MLP model still falls far behind the Bi-LSTM model. This comparison proves that the Bi-LSTM indeed empowers the entity detection ability. 

In order to quantify the errors that Uni-LSTM or MLP makes, we conduct further error analysis. We select those examples that Bi-LSTM makes correct predictions and but Uni-LSTM or MLP fails, and further categorize them into 4 types, as discussed below.


\begin{itemize}
	\item \textbf{Type A}: Wrongly predicting in-vocabulary words as entity. For example,\textit{ For \textbf{more} information check out : www.taconet.com.tw/cstudio/.} The model falsely predicts \textbf{more} as entity.
	\item \textbf{Type B}: Wrongly predicting out-of-vocabulary words as entity. For example, \textit{Another popular technique involves covering the trees with black \textbf{mosquito} nets to improve yields.} The model falsely predicts \textbf{mosquito} as entity while it is an out-of-vocabulary word.
	\item \textbf{Type C}: Failed to predict in-vocabulary words as entity. For example, \textit{Qichen Qian expressed welcome over the visit by vice \textbf{Foreign} Affairs Minister Kamyao and the political consultations in progress}. The model miss to predict an in-vocabulary word \textit{Foreign} as a part of entity.
	\item \textbf{Type D}: Failed to predict out-of-vocabulary words as entity. For example, \textit{\textbf{Guofang} Shen said that this bill is a bill carrying political discrimination and extremely unfriendly to China}. The model miss to predict an out-of-vocabulary word \textbf{Guofang} as a part of entity.
\end{itemize}

\begin{table}[t]
	\centering
	\begin{tabular}{l@{\qquad}cccc}
		\toprule
		\textbf{Model}         & Type A & Type B & Type C & Type D \\ \midrule
		uni-LSTM-forward &  239  & 117 & 823 & 214 \\
		uni-LSTM-backward & xxx & xxx & xxx & xxx  \\
		MLP(bi) & 418 & 1675 & 1363 & 0  \\ 
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{Error Analysis }
	\label{res:ner_error}
\end{table}


The detailed statistics is shown in Table~\ref{res:ner_error}. 


\begin{table}[t]
\small
\centering\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{llp{7cm}}
\toprule
Category	& Sample \# case	&examples \\ \midrule

Form					& 2		& \textbf{Thick} fog , rain and high winds are limiting visibility at the site .  \\
OOV					& 11		& Even if you wanted to , you wo n't be reading \textbf{Jermaine} 's account now.\\
Nearby			& 51		&Ah , yes , something called \textbf{a} Star Chamber .\\
Others  		& 9		&And although Pennsylvania and Massachusetts suffered slight declines \textbf{earlier} in the decade , they are growing again .\\

\bottomrule
\end{tabular}}
\vspace{3mm}
\label{tab:bi-uni}
\caption{Statistic of Bi-LSTM and Uni-LSTM comparison.}
\end{table}

\begin{table}[t]
	\small
	\centering\centering\resizebox{\columnwidth}{!}{
		\begin{tabular}{llp{7cm}}
			\toprule
			Category	& Sample \# case	&examples \\ \midrule
			
			Embedding	&13		&  \textbf{Consumer} activists are skeptical. \\
			False	& 46		& I forced myself to refrain from the grief of \textbf{loosing} my beloved car -LRB- mine was liability - only coverage -RRB- and asked the friend to send their family home.\\
			Inner 	& 20		& It says that what \textbf{the} North Koreans did was a threat to international peace and security.\\
			Others	& 16		& It achieved a total volume of 4 billion US dollars of goods going into and out of the area , increasing by 83 \% over \textbf{the same period of the previous} year , with a total amount of 1 billion US dollars from imports and exports.\\
			\bottomrule
	\end{tabular}}
	\vspace{3mm}
	\label{tab:bi-mlp}
	\caption{Statistic of Bi-LSTM and MLP comparison.}
\end{table}

We also sample 50 sentences from the pool where Bi-LSTM makes correct predictions while uni-LSTM-forward makes wrong predictions, to further examine the error sources. For MLP model, we also sample 50 sentences for error analysis.

We categorize the error sources into 4 types, as shown below.

\begin{itemize}
	\item Form: in some cases, words will be in different form. For example, the first character of the first word will be capitalized. Since Glove is a case sensitive embedding, the two form will have different embedding vectors, which will lead to difference in inference phrase. 
	\item OOV: some words are not in the Glove vocabulary TODO
\end{itemize}









\subsection{Question 2}

In question 1, we have empirically shown that the LSTM model could perform significantly better than local model, then we also want to show this in more theoretically way. With LSTM decomposition, we find that the LSTM can correct several errors made by local model such as MLP. 


%OET  The interview did n't last long , just two hours and it was over .

\subsection{Question 3}

\noindent \textbf{A Motivating Example} 

To examine the source of effectiveness of LSTM, we conduct an experiment where the entities are randomly masked in test set. To eliminate the effect of word embedding and to measure the contribution from context, we mask the entities to ``$<$UNK$>$''. The results are shown in \ref{tab:mask}.

\begin{table}[t]
	\centering
	\begin{tabular}{l@{\qquad}ccc}
		\toprule
		\textbf{Model}		& \textbf{Precision}	& \textbf{Recall}	& \textbf{F1} \\ \midrule
		Bi-LSTM-masked 	& 64.53~(-20.29) 			& 33.74~(49.86)			& 44.31~(-39.89) \\ 
		Uni-LSTM-forward-masked & 67.01~(-5.46) 		& 27.98~(-45.90) 			& 39.48~(-33.69) \\ 
		Uni-LSTM-backward-masked& - 		& -				& - \\ 
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{Entity Detection performance on masked OntoNotes 5 English Dataset Test set. The numbers in bracket denote the performance degradation compared with the corresponding models.}
	\label{tab:mask}
\end{table}

We can find that the recall drop dramatically after masking the entities to ``$<$UNK$>$''. This means that LSTM fails to detect entities without the embedding information from the word itself. This give us the idea that most effectiveness of sequence tagging may come from the word embedding. So we further analysis the embedding using visualization techniques.

\noindent \textbf{Analysis}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.24\textwidth}
		\includegraphics[width=\linewidth]{glove_embedding_positive_mirror.pdf}
		\caption{Glove Embedding with entity word}
		\label{fig:glove_positive}
	\end{subfigure} \hfil 
	\begin{subfigure}[t]{0.24\textwidth}
		\includegraphics[width=\linewidth]{bi_lstm_gold_positive.pdf}
		\caption{Bi-LSTM Embedding with entity word}
		\label{fig:bi_lstm_gold_positive}
	\end{subfigure} \hfil 
	 \begin{subfigure}[t]{0.24\textwidth}
		\includegraphics[width=\linewidth]{bi_lstm_mlp_positive.pdf}
		\caption{Bi-LSTM Embedding with entity word based on the prediction of MLP model}
		\label{fig:bi_lstm_mlp_positive}
	\end{subfigure}

	\bigskip
	
	\begin{subfigure}[t]{0.24\textwidth}
		\includegraphics[width=\linewidth]{glove_embedding_negative_mirror.pdf}
		\caption{Glove Embedding with non-entity word}
		\label{fig:glove_negative}
	\end{subfigure} \hfil
   \begin{subfigure}[t]{0.24\textwidth}
	   \includegraphics[width=\linewidth]{bi_lstm_gold_negative.pdf}
	   \caption{Bi-LSTM Embedding with non-entity word}
	   \label{fig:bi_lstm_gold_negative}
   \end{subfigure} \hfil
	\begin{subfigure}[t]{0.24\textwidth}
		\includegraphics[width=\linewidth]{bi_lstm_mlp_negative.pdf}
		\caption{Bi-LSTM Embedding with non-entity word based on the prediction of MLP model}
		\label{fig:bi_lstm_mlp_negative}
	\end{subfigure}
	\caption{T-SNE for Word Embedding}
	\label{fig:embedding}
\end{figure}

We can observe that with local model such as MLP, the model can also achieve reasonable performance. More than 60 F1 score can be obtained by simple local model, as shown in Table~\ref{res:ner}. %In this subsection, we are trying to investigate the reason behind this, mainly focusing on the embedding level. 

To further investigate the underlying rationals, we conduct the following analysis: we firstly classify the words into two categories: words belong to at least one entity~(entity word), and words nerve belong to any entity(non-entity word). Then we leverage the t-SNE to decompose the any embeddings into two dimensions with unit axises and visualize it. In \ref{fig:embedding}, red dots denote entity words and grey dots denote non-entity words. 

Here, we decompose two sets of embeddings: original GloVe embeddings, embeddings learned from the BiLSTM.

The decomposition of the original GloVe embedding is shown in Figure~\ref{fig:glove_positive} and Figure~\ref{fig:glove_negative}. With these two figures, we can find that the GloVe embeddings indeed contain some entity-related information, because we can observe that the entity-words somehow group in the left side and non-entity-words distribute on right side. But we still can find that most of the entity words and non-entity words are mixed in the middle of the figure. 

We further decompose the embeddings learned from the BiLSTM model, as shown in Figure~\ref{fig:bi_lstm_gold_positive} and Figure~\ref{fig:bi_lstm_gold_negative}. Compared with the figures from GloVe embeddings, we can observe that the entity-words are clustered tighter in Bi-LSTM based embeddings. This phenomenon indicates that the training process optimize the embeddings representation.

In Figure~\ref{fig:bi_lstm_mlp_positive} and Figure~\ref{fig:bi_lstm_mlp_negative}, based on the decomposition of learned Bi-LSTM embeddings, we assign each dot with the color based on the predictions of MLP(bi): If MLP(bi) predict a word as a part of entity, then that corresponding dot will be colored with red; otherwise, grey will be assigned to that dot. The can find that the MLP(bi) can distinguish 84.95\% of the entity-words from non-entity-words. This proves that the idea that the embeddings indeed contribute significantly on Named Entity Detection task.

%Compare empirically the techniques for complexity, performance, ease of use, etc.
%Analyze and compare (empirically or theoretically) your new approach to existing approaches
%What is the best technique, in terms of what?
%Is any technique good enough to declare the problem solved?
 
\section{Results}



\section{Conclusion}
%Can your new technique effectively tackle the problem?
%What future research do you recommend?

%\section*{Acknowledgement}
%Thank people who have helped or influenced you in this project.

\nocite{*}

\bibliographystyle{unsrtnat}

\bibliography{project}

\end{document}